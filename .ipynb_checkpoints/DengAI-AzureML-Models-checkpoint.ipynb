{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deng AI Azure Models and Training\n",
    "#### Environment and Data Prep for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure-ML-WS loaded\n"
     ]
    }
   ],
   "source": [
    "#Load the workspace from the config file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "print(ws.name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up variable to contain input data folder\n",
    "inputdata_folder='inputdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/create_rfr_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/create_rfr_datasets.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from azureml.core import Workspace,Datastore,Dataset,Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#Set run context and workspace\n",
    "run=Run.get_context()\n",
    "ws=run.experiment.workspace\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "#Define the fields used for each city\n",
    "sj_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    'station_max_temp_c',\n",
    "    'station_min_temp_c',\n",
    "    'cum_rain_prior_24_wks',\n",
    "    'avg_max_temp_prior_22_wks',\n",
    "    'total_cases'\n",
    "]\n",
    "\n",
    "sj_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'station_min_temp_c':0,\n",
    "    'cum_rain_prior_24_wks':46,\n",
    "    'avg_max_temp_prior_22_wks':0,\n",
    "    'total_cases':0\n",
    "}\n",
    "\n",
    "iq_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    'reanalysis_min_air_temp_k',\n",
    "    'station_max_temp_c',\n",
    "    'cum_rain_prior_22_wks',\n",
    "    'total_cases'\n",
    "]\n",
    "\n",
    "iq_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    'reanalysis_min_air_temp_k':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'cum_rain_prior_22_wks':43,\n",
    "    'total_cases':0\n",
    "}\n",
    "\n",
    "#Define a function to retrieve the features to be used in the model for each specific city\n",
    "def get_feature_list(city,lag_names=True):\n",
    "    if city=='sj':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            feature_list=sj_features\n",
    "            for key, value in sj_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in sj_lags.items(): feature_list.append(str(key))\n",
    "    elif city=='iq':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            feature_list=iq_features\n",
    "            for key, value in iq_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in iq_lags.items(): feature_list.append(str(key))\n",
    "                \n",
    "    return feature_list\n",
    "\n",
    "#Define a function to create a set of time-lagged features based on the feature and the desired lag\n",
    "def create_lag_features(df,lag,end_col=0):\n",
    "    for i in range(lag):\n",
    "        df_lag=df.iloc[:,:end_col]\n",
    "        df_lag=df_lag.shift(periods=i)\n",
    "        df=df.join(df_lag,rsuffix='_shift_'+str(i))\n",
    "    \n",
    "    df=df.iloc[lag:,:]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#create sets for each city\n",
    "def prep_for_model(city,lookback):\n",
    "    #get train and test for sj or iq\n",
    "    if city=='sj':\n",
    "        train_all_sj_ds = ws.datasets.get('dengue-train-all-sj-ds')\n",
    "        holdout_all_sj_ds = ws.datasets.get('dengue-holdout-all-sj-ds')\n",
    "        df=train_all_sj_ds.to_pandas_dataframe()\n",
    "        df_h=holdout_all_sj_ds.to_pandas_dataframe()\n",
    "        df_h['total_cases']=0\n",
    "    elif city=='iq':\n",
    "        train_all_iq_ds = ws.datasets.get('dengue-train-all-iq-ds')\n",
    "        holdout_all_iq_ds = ws.datasets.get('dengue-holdout-all-iq-ds')\n",
    "        df=train_all_iq_ds.to_pandas_dataframe()\n",
    "        df_h=holdout_all_iq_ds.to_pandas_dataframe()\n",
    "        df_h['total_cases']=0\n",
    "    \n",
    "    #create single dataset\n",
    "    df_all=df.append(df_h,ignore_index=True)\n",
    "\n",
    "    #Get the lists of features to train and reduce the df to those\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=False)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all[training_feature_list].copy()\n",
    "\n",
    "    #Create lagged data\n",
    "    df_all_lag=create_lag_features(df_all_lag,lag=lookback,end_col=df_all_lag.shape[1])\n",
    "\n",
    "    #Reduce features to just the ones needed for training plus the lagged versions of the features since we need 2d dataset\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=True)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all_lag[training_feature_list].copy()\n",
    "\n",
    "    #Break out the label data so it does not get scaled and the drop the values for holdout since they are all 0\n",
    "    y=df_all_lag['total_cases']\n",
    "    y=y[:df.shape[0]-lookback]\n",
    "    df_all_lag.drop(columns=['total_cases'],inplace=True)\n",
    "\n",
    "    #scale features using desired scaler\n",
    "    scaler=RobustScaler()\n",
    "    df_all_lag=scaler.fit_transform(df_all_lag)\n",
    "\n",
    "    #break out the holdout file from the input file\n",
    "    np_df=df_all_lag[:df.shape[0]-lookback,:]\n",
    "    np_df_h=df_all_lag[df.shape[0]-lookback:,:]\n",
    "\n",
    "    return np_df, np_df_h, y\n",
    "\n",
    "#Create the datasets for each city and save to intermediate data file for model use\n",
    "np_sj,np_sj_h,y_sj=prep_for_model(city='sj',lookback=50)\n",
    "df_sj=pd.DataFrame(np_sj)\n",
    "df_sj_holdout=pd.DataFrame(np_sj_h)\n",
    "df_y_sj=pd.DataFrame(y_sj)\n",
    "\n",
    "# Save prepped data to the PipelineData location for sj\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "train_sj_output_path = os.path.join(output_folder, 'train_sj_scaled.csv')\n",
    "df_sj.to_csv(train_sj_output_path,index=False)\n",
    "\n",
    "test_sj_output_path = os.path.join(output_folder, 'holdout_sj_scaled.csv')\n",
    "df_sj_holdout.to_csv(test_sj_output_path,index=False)\n",
    "\n",
    "y_sj_output_path = os.path.join(output_folder, 'y_sj.csv')\n",
    "df_y_sj.to_csv(y_sj_output_path,index=False)\n",
    "\n",
    "#Create the datasets for each city and save to intermediate data file for model use\n",
    "np_iq,np_iq_h,y_iq=prep_for_model(city='iq',lookback=50)\n",
    "df_iq=pd.DataFrame(np_iq)\n",
    "df_iq_holdout=pd.DataFrame(np_iq_h)\n",
    "df_y_iq=pd.DataFrame(y_iq)\n",
    "\n",
    "# Save prepped data to the PipelineData location for iq\n",
    "train_iq_output_path = os.path.join(output_folder, 'train_iq_scaled.csv')\n",
    "df_iq.to_csv(train_iq_output_path,index=False)\n",
    "\n",
    "test_iq_output_path = os.path.join(output_folder, 'holdout_iq_scaled.csv')\n",
    "df_iq_holdout.to_csv(test_iq_output_path,index=False)\n",
    "\n",
    "y_iq_output_path = os.path.join(output_folder, 'y_iq.csv')\n",
    "df_y_iq.to_csv(y_iq_output_path,index=False)\n",
    "\n",
    "### Create reusable datasets for the scaled holdout data. These will be needed to make predictions once the models are deployed\n",
    "default_ds.upload_files(files=[test_sj_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_test_sj_rfr_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/holdout_sj_scaled.csv'))\n",
    "\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_test_sj_rfr_ds = tab_test_sj_rfr_ds.register(workspace=ws, \n",
    "                            name='test-sj-rfr-ds',\n",
    "                            description='Holdout data scaled for SJ RFR model',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "    \n",
    "\n",
    "default_ds.upload_files(files=[test_iq_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_test_iq_rfr_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/holdout_iq_scaled.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_test_iq_rfr_ds = tab_test_iq_rfr_ds.register(workspace=ws, \n",
    "                            name='test-iq-rfr-ds',\n",
    "                            description='Holdout data scaled for IQ RFR model',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "\n",
    "\n",
    "run.complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Register the Model\n",
    "##### Model for SJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/rfr_train_sj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/rfr_train_sj.py\n",
    "#Import libraries\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "#Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "parser.add_argument('--model_folder',type=str,dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "data_folder = args.folder\n",
    "model_folder= args.model_folder\n",
    "\n",
    "#create a dataframe for each dataset, train and holdout\n",
    "df_sj=pd.read_csv(data_folder+'/train_sj_scaled.csv')\n",
    "df_sj_h=pd.read_csv(data_folder+'/holdout_sj_scaled.csv')\n",
    "df_sj_y=pd.read_csv(data_folder+'/y_sj.csv')\n",
    "\n",
    "#split the training set into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_sj, df_sj_y, test_size=0.30, random_state=0)\n",
    "\n",
    "#create the model\n",
    "rfr=RandomForestRegressor(n_estimators=300,max_depth=10)\n",
    "rfr.fit(x_train,y_train)\n",
    "\n",
    "#score the model\n",
    "score=rfr.score(x_test,y_test)\n",
    "print('SJ score: ',score)\n",
    "run.log('SJ score: ',np.float(score))\n",
    "\n",
    "#calculate MAE\n",
    "y_hat=rfr.predict(x_test)\n",
    "mae=mean_absolute_error(y_hat,y_test)\n",
    "print('SJ MAE: ',mae)\n",
    "run.log('SJ MAE: ',np.float(mae))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "output_path = model_folder + \"/sj_rfr_model.pkl\"\n",
    "joblib.dump(value=rfr, filename=output_path)\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/register_rfr_sj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/register_rfr_sj.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "model_folder = args.model_folder\n",
    "print('Model folder',str(model_folder))\n",
    "\n",
    "#Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "#load and register the model\n",
    "print('Loading model from ' + model_folder)\n",
    "model_file = model_folder + '/sj_rfr_model.pkl'\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'sj_rfr_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model for IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/rfr_train_iq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/rfr_train_iq.py\n",
    "#Import libraries\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "parser.add_argument('--model_folder',type=str,dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "data_folder = args.folder\n",
    "model_folder= args.model_folder\n",
    "\n",
    "#create a dataframe for each dataset, train and holdout\n",
    "df_iq=pd.read_csv(data_folder+'/train_iq_scaled.csv')\n",
    "df_iq_h=pd.read_csv(data_folder+'/holdout_iq_scaled.csv')\n",
    "df_iq_y=pd.read_csv(data_folder+'/y_iq.csv')\n",
    "\n",
    "#split the training set into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_iq, df_iq_y, test_size=0.30, random_state=0)\n",
    "\n",
    "#create the model\n",
    "rfr=RandomForestRegressor(n_estimators=300,max_depth=10)\n",
    "rfr.fit(x_train,y_train)\n",
    "\n",
    "#score the model\n",
    "score=rfr.score(x_test,y_test)\n",
    "print('IQ score: ',score)\n",
    "run.log('IQ score: ',np.float(score))\n",
    "\n",
    "#calculate MAE\n",
    "y_hat=rfr.predict(x_test)\n",
    "mae=mean_absolute_error(y_hat,y_test)\n",
    "print('IQ MAE: ',mae)\n",
    "run.log('IQ MAE: ',np.float(mae))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "output_path = model_folder + \"/iq_rfr_model.pkl\"\n",
    "joblib.dump(value=rfr, filename=output_path)\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/register_rfr_iq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/register_rfr_iq.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "model_folder = args.model_folder\n",
    "print('Model folder',str(model_folder))\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the model\n",
    "print('Loading model from ' + model_folder)\n",
    "model_file = model_folder + '/iq_rfr_model.pkl'\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'iq_rfr_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Environment for Model Pipeline\n",
    "##### Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "#Create a compute cluster if it does not exist\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "\n",
    "cluster_name = \"DS-Comp-Cluster\"\n",
    "\n",
    "try:\n",
    "    #Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    #If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Environment on the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "#Create a Python environment for the experiment\n",
    "dengue_env = Environment(\"dengue-pipeline-env\")\n",
    "dengue_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "dengue_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "#Create a set of package dependencies\n",
    "dengue_packages = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','keras','tensorflow'])\n",
    "\n",
    "#Add the dependencies to the environment\n",
    "dengue_env.python.conda_dependencies = dengue_packages\n",
    "\n",
    "#Register the environment (just in case you want to use it again)\n",
    "dengue_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'dengue-pipeline-env')\n",
    "\n",
    "#Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "#Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "#Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Pipelines\n",
    "##### Pipeline for SJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep,EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "#Create a PipelineData object \n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "data_store=ws.get_default_datastore()\n",
    "dengueAI_datasets=PipelineData('deng_datasets',datastore=data_store)\n",
    "model_folder=PipelineData('model_folder',datastore=data_store)\n",
    "\n",
    "#create estimator to run the model\n",
    "estimator = Estimator(source_directory=inputdata_folder,\n",
    "                        compute_target = pipeline_cluster,\n",
    "                        environment_definition=pipeline_run_config.environment,\n",
    "                        entry_script='rfr_train_sj.py')\n",
    "\n",
    "#Step 1, prepare data for the sj model by creating time-lagged features and scaling data\n",
    "create_rfr_datasets = PythonScriptStep(name = 'Create SJ Datasets for RFR Model',\n",
    "                                       source_directory = inputdata_folder,\n",
    "                                       script_name = 'create_rfr_datasets.py',\n",
    "                                       arguments = ['--folder', dengueAI_datasets],\n",
    "                                       inputs=[],\n",
    "                                       outputs=[dengueAI_datasets],\n",
    "                                       compute_target = pipeline_cluster,\n",
    "                                       runconfig = pipeline_run_config,\n",
    "                                       allow_reuse = True)\n",
    "\n",
    "#Step 2, create and train random forest regressor for sj\n",
    "rfr_train_sj=EstimatorStep(name='Create sj random forest regressor model',\n",
    "                             estimator=estimator,\n",
    "                             estimator_entry_script_arguments = ['--folder',dengueAI_datasets,'--model_folder',model_folder],\n",
    "                             inputs=[dengueAI_datasets],\n",
    "                             outputs=[model_folder],\n",
    "                             compute_target = pipeline_cluster,\n",
    "                             allow_reuse = True)\n",
    "\n",
    "#Step 3, register the model\n",
    "register_rfr_sj=PythonScriptStep(name = 'Register RFR Model for SJ',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'register_rfr_sj.py',\n",
    "                                arguments = ['--model_folder',model_folder],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run SJ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Create SJ Datasets for RFR Model [aff61d4f][2077972e-d39f-4187-aa9c-8171bef548f4], (This step will run and generate new outputs)\n",
      "Created step Create sj random forest regressor model [08cdeb44][1dbc5798-6250-496f-96c4-2347ae8e5420], (This step will run and generate new outputs)Created step Register RFR Model for SJ [2aed1015][7be1dd08-bf58-4eac-a843-cf6e9fe264b9], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun eee3b7dd-fcdb-42eb-bbf4-3a1349075332\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-sj-randomforest-pipeline/runs/eee3b7dd-fcdb-42eb-bbf4-3a1349075332?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: eee3b7dd-fcdb-42eb-bbf4-3a1349075332\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-sj-randomforest-pipeline/runs/eee3b7dd-fcdb-42eb-bbf4-3a1349075332?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "{'runId': 'eee3b7dd-fcdb-42eb-bbf4-3a1349075332', 'status': 'Completed', 'startTimeUtc': '2020-11-16T14:17:23.206237Z', 'endTimeUtc': '2020-11-16T14:20:47.607307Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.eee3b7dd-fcdb-42eb-bbf4-3a1349075332/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=J05bHWnumHfqLT%2BNNoYskmUX72rjP%2BJwKzbAdKzp4JM%3D&st=2020-11-16T14%3A09%3A10Z&se=2020-11-16T22%3A19%3A10Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.eee3b7dd-fcdb-42eb-bbf4-3a1349075332/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=svHiialaKCZksTJg%2B0vF75jFP%2FSKEHs92FYpwREmnVo%3D&st=2020-11-16T14%3A09%3A10Z&se=2020-11-16T22%3A19%3A10Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.eee3b7dd-fcdb-42eb-bbf4-3a1349075332/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=2enpYI3nXksNbZ3tYUIeNnzsJSTwR1vDNXQteKEa0N4%3D&st=2020-11-16T14%3A09%3A10Z&se=2020-11-16T22%3A19%3A10Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Construct the pipeline\n",
    "pipeline_steps=[create_rfr_datasets,rfr_train_sj,register_rfr_sj]\n",
    "pipeline = Pipeline(workspace=ws,steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment=Experiment(workspace=ws, name='dengue-sj-randomforest-pipeline')\n",
    "pipeline_run=experiment.submit(pipeline,regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Pipeline for IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep,EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "#Create a PipelineData object \n",
    "ws=Workspace.from_config(path='.azureml/ws_config.json')\n",
    "data_store=ws.get_default_datastore()\n",
    "dengueAI_datasets=PipelineData('deng_datasets',datastore=data_store)\n",
    "model_folder=PipelineData('model_folder',datastore=data_store)\n",
    "\n",
    "\n",
    "#create estimator to run the model\n",
    "estimator = Estimator(source_directory=inputdata_folder,\n",
    "                        compute_target = pipeline_cluster,\n",
    "                        environment_definition=pipeline_run_config.environment,\n",
    "                        entry_script='rfr_train_iq.py')\n",
    "\n",
    "#Step 1, prepare data for the sj model by creating time-lagged features and scaling data\n",
    "create_rfr_datasets = PythonScriptStep(name = 'Create IQ Datasets for RFR Model',\n",
    "                                       source_directory = inputdata_folder,\n",
    "                                       script_name = 'create_rfr_datasets.py',\n",
    "                                       arguments = ['--folder', dengueAI_datasets],\n",
    "                                       inputs=[],\n",
    "                                       outputs=[dengueAI_datasets],\n",
    "                                       compute_target = pipeline_cluster,\n",
    "                                       runconfig = pipeline_run_config,\n",
    "                                       allow_reuse = True)\n",
    "\n",
    "#Step 2, create and train random forest regressor for sj\n",
    "rfr_train_iq = EstimatorStep(name = 'Create iq random forest regressor model',\n",
    "                             estimator=estimator,\n",
    "                             estimator_entry_script_arguments = ['--folder',dengueAI_datasets,'--model_folder',model_folder],\n",
    "                             inputs=[dengueAI_datasets],\n",
    "                             outputs=[model_folder],\n",
    "                             compute_target = pipeline_cluster,\n",
    "                             allow_reuse = True)\n",
    "\n",
    "#Step 3, register the model\n",
    "register_rfr_iq = PythonScriptStep(name = 'Register RFR Model for IQ',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'register_rfr_iq.py',\n",
    "                                arguments = ['--model_folder', model_folder],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run IQ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Create IQ Datasets for RFR Model [946fac25][be1a735d-9313-47bc-9f45-329ef79c5af0], (This step will run and generate new outputs)Created step Create iq random forest regressor model [863a065f][cf8f34ab-b3cb-4830-a27d-e5e4ee151862], (This step will run and generate new outputs)\n",
      "Created step Register RFR Model for IQ [35b14d51][452b039a-19d7-4339-9685-dae6bff9262f], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-iq-randomforest-pipeline/runs/fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-iq-randomforest-pipeline/runs/fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "{'runId': 'fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8', 'status': 'Completed', 'startTimeUtc': '2020-11-16T16:27:39.757008Z', 'endTimeUtc': '2020-11-16T16:30:49.265517Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=tjWjhwSxrNHZ8K87nZ7JWZe0sFRgzvoInQIwWhg0kiA%3D&st=2020-11-16T16%3A20%3A50Z&se=2020-11-17T00%3A30%3A50Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=swmPFVDUI0T%2FMhOHtxWbPbVyruVUWxV4f6T0a9YvsUM%3D&st=2020-11-16T16%3A20%3A50Z&se=2020-11-17T00%3A30%3A50Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.fcb8a3c6-ba27-46d9-9cfe-d9e2f21d7ff8/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=mNOvRVtvojWCbya1LLSDoVCk1CZfhQV1ZTDUHuZc5Ak%3D&st=2020-11-16T16%3A20%3A50Z&se=2020-11-17T00%3A30%3A50Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Construct the pipeline\n",
    "pipeline_steps=[create_rfr_datasets,rfr_train_iq,register_rfr_iq]\n",
    "pipeline = Pipeline(workspace=ws,steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment=Experiment(workspace=ws, name='dengue-iq-randomforest-pipeline')\n",
    "pipeline_run=experiment.submit(pipeline,regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Models as an ACI Service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sj_rfr_model version 6\n",
      "iq_rfr_model version 6\n"
     ]
    }
   ],
   "source": [
    "#Check to see that the model is there in the workspace\n",
    "ws=Workspace.from_config(path='.azureml/ws_config.json')\n",
    "sj_model = ws.models['sj_rfr_model']\n",
    "iq_model=ws.models['iq_rfr_model']\n",
    "print(sj_model.name, 'version', model.version)\n",
    "print(iq_model.name, 'version', model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dengue_service folder created.\n"
     ]
    }
   ],
   "source": [
    "#Create local folder to hold deployment scripts\n",
    "import os\n",
    "\n",
    "folder_name='dengue_service'\n",
    "\n",
    "#Folder for the web service files\n",
    "service_folder = './' + folder_name\n",
    "os.makedirs(service_folder, exist_ok=True)\n",
    "\n",
    "print(folder_name, 'folder created.')\n",
    "\n",
    "#Set path for scoring script\n",
    "sj_script_file=os.path.join(service_folder,'score_rfr_sj.py')\n",
    "iq_script_file=os.path.join(service_folder,'score_rfr_iq.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SJ Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dengue_service\\score_rfr_sj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $sj_script_file\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "#Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    #Get the path to the deployed model file and load it\n",
    "    model_path=Model.get_model_path('sj_rfr_model')\n",
    "    model=joblib.load(model_path)\n",
    "\n",
    "#Called when a request is received\n",
    "def run(raw_data):\n",
    "    #Get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    data = json.loads(raw_data)['data']\n",
    "    \n",
    "    #Get a prediction from the model. This will be a single week's case count\n",
    "    predictions = (model.predict(data))\n",
    "    predictions=predictions.tolist()\n",
    "    \n",
    "    #Return the predictions as JSON\n",
    "    return json.dumps(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency info in ./dengue_service\\dengue_env.yml\n",
      "# Conda environment specification. The dependencies defined in this file will\n",
      "\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "\n",
      "  - azureml-defaults\n",
      "\n",
      "- scikit-learn\n",
      "- pandas\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Environment config for the compute hosting the ACI service\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "import os\n",
    "\n",
    "#Add the dependencies for the model\n",
    "myenv=CondaDependencies()\n",
    "myenv.add_conda_package('scikit-learn')\n",
    "myenv.add_conda_package('pandas')\n",
    "\n",
    "#Save the environment config\n",
    "env_file = os.path.join(experiment_folder,\"dengue_env.yml\")\n",
    "with open(env_file,'w') as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "print(\"Dependency info in\", env_file)\n",
    "\n",
    "#Print the env config file\n",
    "with open(env_file,\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running......................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "#script to actually deploy the service\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "\n",
    "#Config the scoring environment\n",
    "inference_config=InferenceConfig(runtime='python',\n",
    "                                   entry_script=sj_script_file,\n",
    "                                   conda_file=env_file)\n",
    "\n",
    "deployment_config=AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service_name='dengue-sj-rfr-service'\n",
    "\n",
    "service=Model.deploy(ws,service_name,[sj_model],inference_config,deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the service\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IQ Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dengue_service\\score_rfr_iq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $iq_script_file\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "\n",
    "#Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    #Get the path to the deployed model file and load it\n",
    "    model_path=Model.get_model_path('iq_rfr_model')\n",
    "    model=joblib.load(model_path)\n",
    "\n",
    "#Called when a request is received\n",
    "def run(raw_data):\n",
    "    #Get the input data as a numpy array\n",
    "    data=np.array(json.loads(raw_data)['data'])\n",
    "    data=json.loads(raw_data)['data']\n",
    "    \n",
    "    #Get a prediction from the model. This will be a single week's case count\n",
    "    predictions=model.predict(data)\n",
    "    predictions=predictions.tolist()\n",
    "    \n",
    "    #Return the predictions as JSON\n",
    "    return json.dumps(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running....................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "#script to actually deploy the service\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "\n",
    "#Config the scoring environment\n",
    "inference_config=InferenceConfig(runtime='python',\n",
    "                                   entry_script=iq_script_file,\n",
    "                                   conda_file=env_file)\n",
    "\n",
    "deployment_config=AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "\n",
    "service_name='dengue-iq-rfr-service'\n",
    "\n",
    "service=Model.deploy(ws,service_name,[iq_model],inference_config,deployment_config)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consume Services and Make Predictions\n",
    "##### Make Predictions with SJ Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.18.0 to work with Azure-ML-WS\n",
      "260  predictions saved to file.\n"
     ]
    }
   ],
   "source": [
    "endpoint='http://bf0b8018-470a-4970-913c-ae8ab1965e73.centralus.azurecontainer.io/score'\n",
    "key='ThLNPQHmamCIm1jwFXKhn7NqQLEOMwhh'\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from azureml.core import Webservice\n",
    "#get the current workspace\n",
    "ws=Workspace.from_config(path='.azureml/ws_config.json')\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "service=Webservice(ws,'dengue-sj-rfr-service')\n",
    "\n",
    "#get the datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#from the datastore, pull in the dengue test dataset\n",
    "ds_h=ws.datasets.get('test-sj-rfr-ds')\n",
    "\n",
    "#create a dataframe from the test dataset\n",
    "df_h=ds_h.to_pandas_dataframe()\n",
    "\n",
    "#create array to serialize to json\n",
    "js_h=df_h.values.tolist()\n",
    "\n",
    "#Convert the array to JSON format\n",
    "input_json=json.dumps({\"data\": js_h})\n",
    "\n",
    "#Call the web service directly, passing the input data. Doing it this way will provide detailed error message for debugging\n",
    "#response = service.run(input_data = input_json)\n",
    "# Get the predictions\n",
    "#predictions = json.loads(response)\n",
    "#print(len(predictions))\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "#for i in range(len(js_h)):\n",
    "#    print (predictions[i])\n",
    "\n",
    "#Call the webservice via REST\n",
    "#Set the content type and authentication for the request\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key }\n",
    "\n",
    "#Send the request\n",
    "response=requests.post(endpoint, input_json, headers=request_headers)\n",
    "\n",
    "#If we got a valid response, display the predictions\n",
    "if response.status_code == 200:\n",
    "    y_pred=[]\n",
    "    y = json.loads(response.json())\n",
    "    for i in range(len(js_h)):\n",
    "        y_pred.append(int(y[i]))\n",
    "    y_pred=pd.DataFrame(y_pred)\n",
    "    y_pred.to_csv('outputdata/sj_y_pred.csv',index=False)\n",
    "    print(len(y_pred),' predictions saved to file.')\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Predictions with IQ Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.18.0 to work with Azure-ML-WS\n",
      "156  predictions saved to file.\n"
     ]
    }
   ],
   "source": [
    "endpoint='http://c1ea61c5-aaac-4b70-9a3c-03a045b8ea72.centralus.azurecontainer.io/score'\n",
    "key='ZIjvckMqZNDbqIb1Fufj9HlxDhkVSthU'\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from azureml.core import Webservice\n",
    "#get the current workspace\n",
    "ws=Workspace.from_config(path='.azureml/ws_config.json')\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n",
    "service=Webservice(ws,'dengue-iq-rfr-service')\n",
    "\n",
    "#get the datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#from the datastore, pull in the dengue test dataset\n",
    "ds_h=ws.datasets.get('test-iq-rfr-ds')\n",
    "\n",
    "#create a dataframe from the test dataset\n",
    "df_h=ds_h.to_pandas_dataframe()\n",
    "\n",
    "#create array to serialize to json\n",
    "js_h=df_h.values.tolist()\n",
    "\n",
    "#Convert the array to JSON format\n",
    "input_json=json.dumps({\"data\": js_h})\n",
    "\n",
    "#Call the web service directly, passing the input data. Doing it this way will provide detailed error message for debugging\n",
    "#response = service.run(input_data = input_json)\n",
    "# Get the predictions\n",
    "#predictions = json.loads(response)\n",
    "#print(len(predictions))\n",
    "\n",
    "# Print the predicted class for each case.\n",
    "#for i in range(len(js_h)):\n",
    "#    print (predictions[i])\n",
    "\n",
    "#Call the webservice via REST\n",
    "#Set the content type and authentication for the request\n",
    "request_headers = { \"Content-Type\":\"application/json\",\n",
    "                    \"Authorization\":\"Bearer \" + key }\n",
    "\n",
    "#Send the request\n",
    "response=requests.post(endpoint, input_json, headers=request_headers)\n",
    "\n",
    "#If we got a valid response, display the predictions\n",
    "if response.status_code == 200:\n",
    "    y_pred=[]\n",
    "    y = json.loads(response.json())\n",
    "    for i in range(len(js_h)):\n",
    "        y_pred.append(int(y[i]))\n",
    "    y_pred=pd.DataFrame(y_pred)\n",
    "    y_pred.to_csv('outputdata/iq_y_pred.csv',index=False)\n",
    "    print(len(y_pred),' predictions saved to file.')\n",
    "else:\n",
    "    print(response)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Files for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "sj_model='sj-rfr'\n",
    "iq_model='iq-rfr'\n",
    "stamp=date.today().isoformat()\n",
    "\n",
    "#Open the dowloaded prediction files for each city\n",
    "sj_pred=pd.read_csv('outputdata/sj_y_pred.csv')\n",
    "iq_pred=pd.read_csv('outputdata/iq_y_pred.csv')\n",
    "df_submit=pd.read_csv('outputdata/submit_file.csv')\n",
    "\n",
    "#create a single set of predictions\n",
    "y_hat=np.array(sj_pred.append(iq_pred))\n",
    "\n",
    "#add preds to the submit file\n",
    "df_submit['total_cases']=y_hat\n",
    "\n",
    "#save the submission file\n",
    "df_submit.to_csv('outputdata/submit_file_'+sj_model+'_'+iq_model+'_'+stamp+'.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
