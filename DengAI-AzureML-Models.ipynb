{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deng AI Azure Models and Training\n",
    "#### Environment and Data Prep for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure-ML-WS loaded\n"
     ]
    }
   ],
   "source": [
    "#Load the workspace from the config file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "print(ws.name, \"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up variable to contain input data folder\n",
    "inputdata_folder='inputdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inputdata/create_rfr_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/create_rfr_datasets.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from azureml.core import Workspace,Datastore,Dataset,Run\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#Set run context and workspace\n",
    "run=Run.get_context()\n",
    "ws=run.experiment.workspace\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "#Define the fields used for each city\n",
    "sj_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    'station_max_temp_c',\n",
    "    'station_min_temp_c',\n",
    "    'cum_rain_prior_24_wks',\n",
    "    'avg_max_temp_prior_22_wks',\n",
    "    'total_cases'\n",
    "]\n",
    "\n",
    "sj_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'station_min_temp_c':0,\n",
    "    'cum_rain_prior_24_wks':46,\n",
    "    'avg_max_temp_prior_22_wks':0,\n",
    "    'total_cases':0\n",
    "}\n",
    "\n",
    "iq_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    'reanalysis_min_air_temp_k',\n",
    "    'station_max_temp_c',\n",
    "    'cum_rain_prior_22_wks',\n",
    "    'total_cases'\n",
    "]\n",
    "\n",
    "iq_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    'reanalysis_min_air_temp_k':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'cum_rain_prior_22_wks':43,\n",
    "    'total_cases':0\n",
    "}\n",
    "\n",
    "#Define a function to retrieve the features to be used in the model for each specific city\n",
    "def get_feature_list(city,lag_names=True):\n",
    "    if city=='sj':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            feature_list=sj_features\n",
    "            for key, value in sj_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in sj_lags.items(): feature_list.append(str(key))\n",
    "    elif city=='iq':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            feature_list=iq_features\n",
    "            for key, value in iq_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in iq_lags.items(): feature_list.append(str(key))\n",
    "                \n",
    "    return feature_list\n",
    "\n",
    "#Define a function to create a set of time-lagged features based on the feature and the desired lag\n",
    "def create_lag_features(df,lag,end_col=0):\n",
    "    for i in range(lag):\n",
    "        df_lag=df.iloc[:,:end_col]\n",
    "        df_lag=df_lag.shift(periods=i)\n",
    "        df=df.join(df_lag,rsuffix='_shift_'+str(i))\n",
    "    \n",
    "    df=df.iloc[lag:,:]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#create sets for each city\n",
    "def prep_for_model(city,lookback):\n",
    "    #get train and test for sj or iq\n",
    "    if city=='sj':\n",
    "        #df=pd.read_csv('inputdata/train_all_sj.csv')\n",
    "        #df_h=pd.read_csv('inputdata/holdout_all_sj.csv')\n",
    "        train_all_sj_ds = ws.datasets.get('dengue-train-all-sj-ds')\n",
    "        holdout_all_sj_ds = ws.datasets.get('dengue-holdout-all-sj-ds')\n",
    "        df=train_all_sj_ds.to_pandas_dataframe()\n",
    "        df_h=holdout_all_sj_ds.to_pandas_dataframe()\n",
    "        df_h['total_cases']=0\n",
    "    elif city=='iq':\n",
    "        #df=pd.read_csv('inputdata/train_all_iq.csv')\n",
    "        #df_h=pd.read_csv('inputdata/holdout_all_iq.csv')\n",
    "        train_all_iq_ds = ws.datasets.get('dengue-train-all-iq-ds')\n",
    "        holdout_all_iq_ds = ws.datasets.get('dengue-holdout-all-iq-ds')\n",
    "        df=train_all_iq_ds.to_pandas_dataframe()\n",
    "        df_h=holdout_all_iq_ds.to_pandas_dataframe()\n",
    "        df_h['total_cases']=0\n",
    "    \n",
    "    #create single dataset\n",
    "    df_all=df.append(df_h,ignore_index=True)\n",
    "\n",
    "    #Get the lists of features to train and reduce the df to those\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=False)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all[training_feature_list].copy()\n",
    "\n",
    "    #Create lagged data\n",
    "    df_all_lag=create_lag_features(df_all_lag,lag=lookback,end_col=df_all_lag.shape[1])\n",
    "\n",
    "    #Reduce features to just the ones needed for training plus the lagged versions of the features since we need 2d dataset\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=True)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all_lag[training_feature_list].copy()\n",
    "\n",
    "    #Break out the label data so it does not get scaled and the drop the values for holdout since they are all 0\n",
    "    y=df_all_lag['total_cases']\n",
    "    y=y[:df.shape[0]]\n",
    "    df_all_lag.drop(columns=['total_cases'],inplace=True)\n",
    "\n",
    "    #scale features using desired scaler\n",
    "    scaler=RobustScaler()\n",
    "    df_all_lag=scaler.fit_transform(df_all_lag)\n",
    "\n",
    "    #break out the holdout file from the input file\n",
    "    np_df=df_all_lag[:df.shape[0],:]\n",
    "    np_df_h=df_all_lag[df.shape[0]:,:]\n",
    "\n",
    "    return np_df, np_df_h, y\n",
    "\n",
    "#Create the datasets for each city and save to intermediate data file for model use\n",
    "np_sj,np_sj_h,y_sj=prep_for_model(city='sj',lookback=50)\n",
    "df_sj=pd.DataFrame(np_sj)\n",
    "df_sj_holdout=pd.DataFrame(np_sj_h)\n",
    "df_y_sj=pd.DataFrame(y_sj)\n",
    "\n",
    "# Save prepped data to the PipelineData location for sj\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "train_sj_output_path = os.path.join(output_folder, 'train_sj_scaled.csv')\n",
    "df_sj.to_csv(train_sj_output_path,index=False)\n",
    "\n",
    "test_sj_output_path = os.path.join(output_folder, 'holdout_sj_scaled.csv')\n",
    "df_sj_holdout.to_csv(test_sj_output_path,index=False)\n",
    "\n",
    "y_sj_output_path = os.path.join(output_folder, 'y_sj.csv')\n",
    "df_y_sj.to_csv(y_sj_output_path,index=False)\n",
    "\n",
    "#Create the datasets for each city and save to intermediate data file for model use\n",
    "np_iq,np_iq_h,y_iq=prep_for_model(city='iq',lookback=50)\n",
    "df_iq=pd.DataFrame(np_iq)\n",
    "df_iq_holdout=pd.DataFrame(np_iq_h)\n",
    "df_y_iq=pd.DataFrame(y_iq)\n",
    "\n",
    "# Save prepped data to the PipelineData location for iq\n",
    "train_iq_output_path = os.path.join(output_folder, 'train_iq_scaled.csv')\n",
    "df_iq.to_csv(train_iq_output_path,index=False)\n",
    "\n",
    "test_iq_output_path = os.path.join(output_folder, 'holdout_iq_scaled.csv')\n",
    "df_iq_holdout.to_csv(test_iq_output_path,index=False)\n",
    "\n",
    "y_iq_output_path = os.path.join(output_folder, 'y_iq.csv')\n",
    "df_y_iq.to_csv(y_iq_output_path,index=False)\n",
    "\n",
    "run.complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Register the Model\n",
    "##### Model for SJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/rfr_train_sj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/rfr_train_sj.py\n",
    "#Import libraries\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "parser.add_argument('--model_folder',type=str,dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "data_folder = args.folder\n",
    "model_folder= args.model_folder\n",
    "\n",
    "#create a dataframe for each dataset, train and holdout\n",
    "df_sj=pd.read_csv(data_folder+'/train_sj_scaled.csv')\n",
    "df_sj_h=pd.read_csv(data_folder+'/holdout_sj_scaled.csv')\n",
    "df_sj_y=pd.read_csv(data_folder+'/y_sj.csv')\n",
    "\n",
    "#get the datasets for the city\n",
    "#np_sj,np_sj_h,y_sj=prep_for_model(city='sj',lookback=50)\n",
    "\n",
    "#split the training set into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_sj, df_sj_y, test_size=0.30, random_state=0)\n",
    "\n",
    "#create the model\n",
    "rfr=RandomForestRegressor(n_estimators=300,max_depth=10)\n",
    "rfr.fit(x_train,y_train)\n",
    "\n",
    "#score the model\n",
    "score=rfr.score(x_test,y_test)\n",
    "print('SJ score: ',score)\n",
    "run.log('SJ score: ',np.float(score))\n",
    "\n",
    "#calculate MAE\n",
    "y_hat=rfr.predict(x_test)\n",
    "mae=mean_absolute_error(y_hat,y_test)\n",
    "print('SJ MAE: ',mae)\n",
    "run.log('SJ MAE: ',np.float(mae))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "output_path = model_folder + \"/sj_rfr_model.pkl\"\n",
    "joblib.dump(value=rfr, filename=output_path)\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/register_rfr_sj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/register_rfr_sj.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "model_folder = args.model_folder\n",
    "print('Model folder',str(model_folder))\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the model\n",
    "print('Loading model from '' + model_folder)\n",
    "model_file = model_folder + '/sj_rfr_model.pkl'\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'sj_rfr_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model for IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inputdata/rfr_train_iq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/rfr_train_iq.py\n",
    "#Import libraries\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "parser.add_argument('--model_folder',type=str,dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "data_folder = args.folder\n",
    "model_folder= args.model_folder\n",
    "\n",
    "#create a dataframe for each dataset, train and holdout\n",
    "df_iq=pd.read_csv(data_folder+'/train_iq_scaled.csv')\n",
    "df_iq_h=pd.read_csv(data_folder+'/holdout_iq_scaled.csv')\n",
    "df_iq_y=pd.read_csv(data_folder+'/y_iq.csv')\n",
    "\n",
    "#split the training set into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_iq, df_iq_y, test_size=0.30, random_state=0)\n",
    "\n",
    "#create the model\n",
    "rfr=RandomForestRegressor(n_estimators=300,max_depth=10)\n",
    "rfr.fit(x_train,y_train)\n",
    "\n",
    "#score the model\n",
    "score=rfr.score(x_test,y_test)\n",
    "print('IQ score: ',score)\n",
    "run.log('IQ score: ',np.float(score))\n",
    "\n",
    "#calculate MAE\n",
    "y_hat=rfr.predict(x_test)\n",
    "mae=mean_absolute_error(y_hat,y_test)\n",
    "print('IQ MAE: ',mae)\n",
    "run.log('IQ MAE: ',np.float(mae))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "output_path = model_folder + \"/iq_rfr_model.pkl\"\n",
    "joblib.dump(value=rfr, filename=output_path)\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/register_rfr_iq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/register_rfr_iq.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import joblib\n",
    "from azureml.core import Workspace, Model, Run\n",
    "\n",
    "# Get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_folder', type=str, dest='model_folder')\n",
    "args = parser.parse_args()\n",
    "model_folder = args.model_folder\n",
    "print('Model folder',str(model_folder))\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the model\n",
    "print('Loading model from ' + model_folder)\n",
    "model_file = model_folder + '/iq_rfr_model.pkl'\n",
    "model = joblib.load(model_file)\n",
    "\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "               model_path = model_file,\n",
    "               model_name = 'iq_rfr_model',\n",
    "               tags={'Training context':'Pipeline'})\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Environment for Model Pipeline\n",
    "##### Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "#Create a compute cluster if it does not exist\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "\n",
    "cluster_name = \"DS-Comp-Cluster\"\n",
    "\n",
    "try:\n",
    "    #Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    #If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Environment on the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "dengue_env = Environment(\"dengue-pipeline-env\")\n",
    "dengue_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "dengue_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies\n",
    "dengue_packages = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','keras','tensorflow'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "dengue_env.python.conda_dependencies = dengue_packages\n",
    "\n",
    "# Register the environment (just in case you want to use it again)\n",
    "dengue_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'dengue-pipeline-env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Pipelines\n",
    "##### Pipeline for SJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep,EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "#Create a PipelineData object \n",
    "ws=Workspace.get(name='Azure-ML-WS',subscription_id='fd2d8de8-17e1-4976-9906-fdde487edd5f',resource_group='AzureML-Learning')\n",
    "data_store=ws.get_default_datastore()\n",
    "dengueAI_datasets=PipelineData('deng_datasets',datastore=data_store)\n",
    "model_folder=PipelineData('model_folder',datastore=data_store)\n",
    "\n",
    "\n",
    "#create estimator to run the model\n",
    "estimator = Estimator(source_directory=inputdata_folder,\n",
    "                        compute_target = pipeline_cluster,\n",
    "                        environment_definition=pipeline_run_config.environment,\n",
    "                        entry_script='rfr_train_sj.py')\n",
    "\n",
    "#Step 1, prepare data for the sj model by creating time-lagged features and scaling data\n",
    "create_rfr_datasets = PythonScriptStep(name = 'Create SJ Datasets for RFR Model',\n",
    "                                       source_directory = inputdata_folder,\n",
    "                                       script_name = 'create_rfr_datasets.py',\n",
    "                                       arguments = ['--folder', dengueAI_datasets],\n",
    "                                       inputs=[],\n",
    "                                       outputs=[dengueAI_datasets],\n",
    "                                       compute_target = pipeline_cluster,\n",
    "                                       runconfig = pipeline_run_config,\n",
    "                                       allow_reuse = True)\n",
    "\n",
    "#Step 2, create and train random forest regressor for sj\n",
    "rfr_train_sj = EstimatorStep(name = 'Create sj random forest regressor model',\n",
    "                             estimator=estimator,\n",
    "                             estimator_entry_script_arguments = ['--folder',dengueAI_datasets,'--model_folder',model_folder],\n",
    "                             inputs=[dengueAI_datasets],\n",
    "                             outputs=[model_folder],\n",
    "                             compute_target = pipeline_cluster,\n",
    "                             allow_reuse = True)\n",
    "\n",
    "#Step 3, register the model\n",
    "register_rfr_sj = PythonScriptStep(name = 'Register RFR Model for SJ',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'register_rfr_sj.py',\n",
    "                                arguments = ['--model_folder', model_folder],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run SJ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Create IQ Datasets for RFR Model [44367085][9e94323e-a224-41ac-8838-ac898b9d60dc], (This step will run and generate new outputs)\n",
      "Created step Create sj random forest regressor model [c69a2562][ab0a3a1e-4397-481b-935d-f52ae914d9c8], (This step will run and generate new outputs)\n",
      "Created step Create SJ Datasets for RFR Model [7c114c18][27169125-fbed-4eda-8530-ad9d0c877968], (This step will run and generate new outputs)\n",
      "Created step Register RFR Model for SJ [665d79a3][5095e1d0-e5a2-488c-8756-7f4740e71a77], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 82151df3-f78f-49cf-9c06-ec181271bd44\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-sj-randomforest-pipeline/runs/82151df3-f78f-49cf-9c06-ec181271bd44?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: 82151df3-f78f-49cf-9c06-ec181271bd44\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-sj-randomforest-pipeline/runs/82151df3-f78f-49cf-9c06-ec181271bd44?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "{'runId': '82151df3-f78f-49cf-9c06-ec181271bd44', 'status': 'Completed', 'startTimeUtc': '2020-11-14T23:51:28.11924Z', 'endTimeUtc': '2020-11-14T23:57:29.071773Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.82151df3-f78f-49cf-9c06-ec181271bd44/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=VlK3kINBhks8gG%2Bmg4qSEcnMHSZmRbIuOtb48a70N90%3D&st=2020-11-14T23%3A47%3A30Z&se=2020-11-15T07%3A57%3A30Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.82151df3-f78f-49cf-9c06-ec181271bd44/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=OPrlkpIpPxNSitHuw2Qe4GsjJAczNhg3TYhX9X692EQ%3D&st=2020-11-14T23%3A47%3A30Z&se=2020-11-15T07%3A57%3A30Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.82151df3-f78f-49cf-9c06-ec181271bd44/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ill3Rf0zO0G6oW0ofv4CSKsAlIngEFLko%2FApRDdg7TU%3D&st=2020-11-14T23%3A47%3A30Z&se=2020-11-15T07%3A57%3A30Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Construct the pipeline\n",
    "pipeline_steps=[create_rfr_datasets,rfr_train_sj,register_rfr_sj]\n",
    "pipeline = Pipeline(workspace=ws,steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment=Experiment(workspace=ws, name='dengue-sj-randomforest-pipeline')\n",
    "pipeline_run=experiment.submit(pipeline,regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Pipeline for IQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep,EstimatorStep\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "#Create a PipelineData object \n",
    "ws=Workspace.from_config(path='.azureml/ws_config.json')\n",
    "data_store=ws.get_default_datastore()\n",
    "dengueAI_datasets=PipelineData('deng_datasets',datastore=data_store)\n",
    "model_folder=PipelineData('model_folder',datastore=data_store)\n",
    "\n",
    "\n",
    "#create estimator to run the model\n",
    "estimator = Estimator(source_directory=inputdata_folder,\n",
    "                        compute_target = pipeline_cluster,\n",
    "                        environment_definition=pipeline_run_config.environment,\n",
    "                        entry_script='rfr_train_iq.py')\n",
    "\n",
    "#Step 1, prepare data for the sj model by creating time-lagged features and scaling data\n",
    "create_rfr_datasets = PythonScriptStep(name = 'Create IQ Datasets for RFR Model',\n",
    "                                       source_directory = inputdata_folder,\n",
    "                                       script_name = 'create_rfr_datasets.py',\n",
    "                                       arguments = ['--folder', dengueAI_datasets],\n",
    "                                       inputs=[],\n",
    "                                       outputs=[dengueAI_datasets],\n",
    "                                       compute_target = pipeline_cluster,\n",
    "                                       runconfig = pipeline_run_config,\n",
    "                                       allow_reuse = True)\n",
    "\n",
    "#Step 2, create and train random forest regressor for sj\n",
    "rfr_train_iq = EstimatorStep(name = 'Create iq random forest regressor model',\n",
    "                             estimator=estimator,\n",
    "                             estimator_entry_script_arguments = ['--folder',dengueAI_datasets,'--model_folder',model_folder],\n",
    "                             inputs=[dengueAI_datasets],\n",
    "                             outputs=[model_folder],\n",
    "                             compute_target = pipeline_cluster,\n",
    "                             allow_reuse = True)\n",
    "\n",
    "#Step 3, register the model\n",
    "register_rfr_iq = PythonScriptStep(name = 'Register RFR Model for IQ',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'register_rfr_iq.py',\n",
    "                                arguments = ['--model_folder', model_folder],\n",
    "                                inputs=[model_folder],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run IQ Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Create IQ Datasets for RFR Model [1c6abb00][068a97dd-396f-44de-8fd1-b0b84af1d930], (This step will run and generate new outputs)\n",
      "Created step Create iq random forest regressor model [bd0cd3dd][42ec92e7-4a6d-4e91-b8a1-426d45964e64], (This step will run and generate new outputs)\n",
      "Created step Register RFR Model for IQ [e89548c4][9733f18f-4faa-4767-a4af-9b56de17e6ea], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-iq-randomforest-pipeline/runs/13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: 13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-iq-randomforest-pipeline/runs/13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "{'runId': '13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5', 'status': 'Failed', 'startTimeUtc': '2020-11-14T23:58:26.919556Z', 'endTimeUtc': '2020-11-15T00:01:51.064747Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=2VChQaQsXrc8r4Tug2cucrpNkYzzrrnGlDZGzQDF%2Bvo%3D&st=2020-11-14T23%3A51%3A52Z&se=2020-11-15T08%3A01%3A52Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=4Qnkc1ZvfvGyNU56ilutsbOjVYYRGj%2FXH%2F2k7BSqCAs%3D&st=2020-11-14T23%3A51%3A52Z&se=2020-11-15T08%3A01%3A52Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.13625d0c-ddb2-4030-a7f9-bac9d2aa1ac5/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=EQwb686j0FOT51kK2XClHeldpTmGLnbBjkXBCT6JXIw%3D&st=2020-11-14T23%3A51%3A52Z&se=2020-11-15T08%3A01%3A52Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Failed'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Construct the pipeline\n",
    "pipeline_steps=[create_rfr_datasets,rfr_train_iq,register_rfr_iq]\n",
    "pipeline = Pipeline(workspace=ws,steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment=Experiment(workspace=ws, name='dengue-iq-randomforest-pipeline')\n",
    "pipeline_run=experiment.submit(pipeline,regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
