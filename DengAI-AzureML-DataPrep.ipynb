{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Notebook for DengAI on AzureML\n",
    "This notebook contains the initial AzureML setup process for the DengAI as well as the programmtic components for DS operations on Azure. This includes Workspace, experiment, pipeline, and endpoint management processes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workspace Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure-ML-WS Config written as ws_config.json\n"
     ]
    }
   ],
   "source": [
    "#Create workspace configuration file - one time code for this project - assumes workspace already exists\n",
    "from azureml.core import Workspace\n",
    "ws=Workspace.get(name='ENTER WS NAME HERE',subscription_id='ENTER SUBSCRIPTION HERE',resource_group='ENTER RG NAME HERE')\n",
    "\n",
    "#write out a config file\n",
    "ws.write_config(file_name=\"ws_config.json\")\n",
    "\n",
    "print(ws.name,'Config written as ws_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure-ML-WS loaded\n"
     ]
    }
   ],
   "source": [
    "#Load the workspace from the config file\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "print(ws.name, \"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Upload, Initial Prep., and Dataset Creation\n",
    "Create datasets for the initial data files for the competition. These files will then be used for further processing in prep to run experiments and data analysis,\n",
    "##### Upload data to datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets - Default = False\n",
      "workspacefilestore - Default = False\n",
      "workspaceblobstore - Default = True\n"
     ]
    }
   ],
   "source": [
    "#Get a list of current datastores in the workspace\n",
    "default_ds=ws.get_default_datastore()\n",
    "\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading inputdata/dengue_features_test.csv\n",
      "Uploaded inputdata/dengue_features_test.csv, 1 files out of an estimated total of 2\n",
      "Uploading inputdata/dengue_features_train.csv\n",
      "Uploaded inputdata/dengue_features_train.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n",
      "$AZUREML_DATAREFERENCE_3db657dad66c43a9acdc149ee6edbc24\n"
     ]
    }
   ],
   "source": [
    "#Upload the source competition files to the datastore\n",
    "default_ds.upload_files(files=['inputdata/dengue_features_train.csv', 'inputdata/dengue_features_test.csv'], # Upload the diabetes csv files\n",
    "                       target_path='dengueAI/inputdata', # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)\n",
    "#Create a data_ref object for the file location\n",
    "data_ref = default_ds.path('dengueAI/inputdata').as_download(path_on_compute='inputdata')\n",
    "print(data_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and Register Datasets from Raw Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up variable to contain input data folder\n",
    "inputdata_folder='inputdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/create_raw_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/create_raw_datasets.py\n",
    "\n",
    "#Import needed libraries\n",
    "from azureml.core import Dataset\n",
    "\n",
    "#Create datasets for the raw training and holdout/test file\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "default_ds.upload_files(files=['inputdata/dengue_features_train.csv'],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_train_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/dengue_features_train.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_train_ds = tab_train_ds.register(workspace=ws, \n",
    "                            name='dengue-features-train-ds',\n",
    "                            description='Raw dengue feature training data',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "    \n",
    "default_ds.upload_files(files=['inputdata/dengue_features_test.csv'],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_test_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/dengue_features_test.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_test_ds = tab_test_ds.register(workspace=ws, \n",
    "                            name='dengue-features-test-ds',\n",
    "                            description='Raw dengue feature test/holdout data',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Data Prep and Dataset Creation\n",
    "Do initial data prep to create a single, clean copy of the training and holdout data, and then create a dataset for the result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/create_clean_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/create_clean_datasets.py\n",
    "\n",
    "from azureml.core import Workspace,Datastore,Dataset,Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "#Get the datasets from the input params\n",
    "df_i=run.input_datasets['dengue_train'].to_pandas_dataframe()\n",
    "df_h=run.input_datasets['dengue_test'].to_pandas_dataframe()\n",
    "\n",
    "#interpolate missing values\n",
    "df_i.interpolate(inplace=True,method='linear',limit_direction='forward')\n",
    "df_h.interpolate(inplace=True,method='linear',limit_direction='forward')\n",
    "\n",
    "#Add a total cases column to the holdout set\n",
    "df_h['total_cases']=0\n",
    "\n",
    "#split by city\n",
    "df_sj=df_i[df_i['city']=='sj']\n",
    "df_iq=df_i[df_i['city']=='iq']\n",
    "\n",
    "df_sj_h=df_h[df_h['city']=='sj']\n",
    "df_iq_h=df_h[df_h['city']=='iq']\n",
    "\n",
    "len_sj=len(df_sj)\n",
    "len_sj_h=len(df_sj_h)\n",
    "\n",
    "#concat training and holdout data\n",
    "df_sj=df_sj.append(df_sj_h)\n",
    "\n",
    "#Get cumulative totals at various intervals - past 4 to past 25 weeks\n",
    "for i in range(2,25):\n",
    "    df_sj['cum_rain_prior_'+str(i)+'_wks']=df_sj['precipitation_amt_mm'].rolling(i).sum()\n",
    "\n",
    "for i in range(2,25):\n",
    "    df_sj['avg_min_temp_prior_'+str(i)+'_wks']=df_sj['station_min_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_max_temp_prior_'+str(i)+'_wks']=df_sj['station_max_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_specific_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_relative_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_relative_humidity_percent'].rolling(i).mean()\n",
    "\n",
    "#split the files back apart\n",
    "df_sj_h=df_sj.iloc[len_sj:]\n",
    "df_sj=df_sj.iloc[:len_sj]\n",
    "\n",
    "#remove the first 25 rows of the input file because of the rolling numbers and the nan's they create\n",
    "df_sj=df_sj.iloc[25:,:]\n",
    "\n",
    "#Begin IQ\n",
    "#Break out source file and holdout file by city\n",
    "len_iq=len(df_iq)\n",
    "len_iq_h=len(df_iq_h)\n",
    "\n",
    "#concat training and holdout data\n",
    "df_iq=df_iq.append(df_iq_h)\n",
    "\n",
    "#Get cumulative rainfall totals at various accumulations - past 4 to past 25 weeks\n",
    "for i in range(2,25):\n",
    "    df_iq['cum_rain_prior_'+str(i)+'_wks']=df_iq['precipitation_amt_mm'].rolling(i).sum()\n",
    "\n",
    "for i in range(2,25):\n",
    "    df_iq['avg_min_temp_prior_'+str(i)+'_wks']=df_iq['station_min_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_max_temp_prior_'+str(i)+'_wks']=df_iq['station_max_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_specific_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_relative_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_relative_humidity_percent'].rolling(i).mean()\n",
    "\n",
    "#split the files back apart\n",
    "df_iq_h=df_iq.iloc[len_iq:]\n",
    "df_iq=df_iq.iloc[:len_iq]\n",
    "\n",
    "#remove the first 25 rows of the input file because of the rolling numbers\n",
    "df_iq=df_iq.iloc[25:,:]\n",
    "\n",
    "#reconstructe df's\n",
    "df_all=df_sj.append(df_iq)\n",
    "df_holdout=df_sj_h.append(df_iq_h)\n",
    "df_holdout.drop(columns=['total_cases'],inplace=True)\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "train_output_path = os.path.join(output_folder, 'dengue_train_all.csv')\n",
    "df_all.to_csv(train_output_path,index=False)\n",
    "test_output_path = os.path.join(output_folder, 'dengue_holdout_all.csv')\n",
    "df_holdout.to_csv(test_output_path,index=False)\n",
    "\n",
    "##Create datasets for the raw training and holdout/test file\n",
    "#Get the default data store\n",
    "ws=run.experiment.workspace\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "default_ds.upload_files(files=[train_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_train_all_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/dengue_train_all.csv'))\n",
    "\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_train_all_ds = tab_train_all_ds.register(workspace=ws, \n",
    "                            name='dengue-features-train-all-ds',\n",
    "                            description='Cleaned dengue feature training data',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "    \n",
    "\n",
    "default_ds.upload_files(files=[test_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_holdout_all_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/dengue_holdout_all.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_holdout_all_ds = tab_holdout_all_ds.register(workspace=ws, \n",
    "                            name='dengue-features-holdout-all-ds',\n",
    "                            description='Cleaned dengue feature test/holdout data',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Pre-processing\n",
    "Create a script to pre-process the data into datasets for each city ready for city-specific lagging, scaling, and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputdata/create_city_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $inputdata_folder/create_city_datasets.py\n",
    "\n",
    "from azureml.core import Workspace,Datastore,Dataset,Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Set run context\n",
    "run=Run.get_context()\n",
    "\n",
    "# Get PipelineData argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', type=str, dest='folder')\n",
    "args = parser.parse_args()\n",
    "output_folder = args.folder\n",
    "\n",
    "#create a dataframe for each dataset, train and holdout\n",
    "df_a=pd.read_csv(output_folder+'/dengue_train_all.csv')\n",
    "df_h=pd.read_csv(output_folder+'/dengue_holdout_all.csv')\n",
    "\n",
    "#create a diferrent df for each city\n",
    "df_sj=df_a[df_a['city']=='sj']\n",
    "df_sj_h=df_h[df_h['city']=='sj']\n",
    "df_iq=df_a[df_a['city']=='iq']\n",
    "df_iq_h=df_h[df_h['city']=='iq']\n",
    "\n",
    "# Save prepped data to the PipelineData location\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "train_sj_output_path = os.path.join(output_folder, 'train_all_sj.csv')\n",
    "df_sj.to_csv(train_sj_output_path,index=False)\n",
    "test_sj_output_path = os.path.join(output_folder, 'holdout_all_sj.csv')\n",
    "df_sj_h.to_csv(test_sj_output_path,index=False)\n",
    "\n",
    "train_iq_output_path = os.path.join(output_folder, 'train_all_iq.csv')\n",
    "df_iq.to_csv(train_iq_output_path,index=False)\n",
    "test_iq_output_path = os.path.join(output_folder, 'holdout_all_iq.csv')\n",
    "df_iq_h.to_csv(test_iq_output_path,index=False)\n",
    "\n",
    "#upload and create datasets\n",
    "#Get the default data store\n",
    "ws=run.experiment.workspace\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "default_ds.upload_files(files=[train_sj_output_path],\n",
    "                        target_path='dengueAI/inputdata',\n",
    "                        overwrite=True, \n",
    "                        show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_train_all_sj_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/train_all_sj.csv'))\n",
    "\n",
    "#Register the tabular dataset for sj train_all\n",
    "try:\n",
    "    tab_train_all_sj_ds = tab_train_all_sj_ds.register(workspace=ws, \n",
    "                            name='dengue-train-all-sj-ds',\n",
    "                            description='Lagged feature training data for sj',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "    \n",
    "default_ds.upload_files(files=[test_sj_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_holdout_all_sj_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/holdout_all_sj.csv'))\n",
    "\n",
    "#Register the tabular dataset for sj holdout all\n",
    "try:\n",
    "    tab_holdout_all_sj_ds = tab_holdout_all_sj_ds.register(workspace=ws, \n",
    "                            name='dengue-holdout-all-sj-ds',\n",
    "                            description='Lagged dengue feature test/holdout data for sj',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    \n",
    "\n",
    "#Create upload for train iq\n",
    "default_ds.upload_files(files=[train_iq_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_train_all_iq_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/train_all_iq.csv'))\n",
    "\n",
    "#Register the tabular dataset for train iq\n",
    "try:\n",
    "    tab_train_all_iq_ds = tab_train_all_iq_ds.register(workspace=ws, \n",
    "                            name='dengue-train-all-iq-ds',\n",
    "                            description='Lagged feature training data for iq',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "    \n",
    "#Create upload for test iq\n",
    "default_ds.upload_files(files=[test_iq_output_path],\n",
    "                    target_path='dengueAI/inputdata',\n",
    "                    overwrite=True, \n",
    "                    show_progress=True)\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore for the file\n",
    "tab_holdout_all_iq_ds = Dataset.Tabular.from_delimited_files(path=(default_ds, 'dengueAI/inputdata/holdout_all_iq.csv'))\n",
    "\n",
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_holdout_all_iq_ds = tab_holdout_all_iq_ds.register(workspace=ws, \n",
    "                            name='dengue-holdout-all-iq-ds',\n",
    "                            description='Lagged dengue feature test/holdout data for iq',\n",
    "                            tags = {'format':'CSV'},\n",
    "                            create_new_version=True)\n",
    "    print('Dataset registered.')\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Compute Environment for Data Prep Pipeline\n",
    "##### Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "#Create a compute cluster if it does not exist\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "\n",
    "cluster_name = \"DS-Comp-Cluster\"\n",
    "\n",
    "try:\n",
    "    #Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    #If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Environment on the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "dengue_env = Environment(\"dengue-pipeline-env\")\n",
    "dengue_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "dengue_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies\n",
    "dengue_packages = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]','keras','tensorflow'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "dengue_env.python.conda_dependencies = dengue_packages\n",
    "\n",
    "# Register the environment (just in case you want to use it again)\n",
    "dengue_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'dengue-pipeline-env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Run the Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps defined\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "#Get the dataset for the initial data files\n",
    "dengue_train_ds = ws.datasets.get('dengue-features-train-ds')\n",
    "dengue_test_ds = ws.datasets.get('dengue-features-test-ds')\n",
    "\n",
    "#Create a PipelineData\n",
    "ws = Workspace.from_config(path='.azureml/ws_config.json')\n",
    "data_store=ws.get_default_datastore()\n",
    "dengueAI=PipelineData('dengueAI',datastore=data_store)\n",
    "\n",
    "#Step 1, clean the raw datasets\n",
    "create_clean_datasets = PythonScriptStep(name = 'Create Clean Datasets',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'create_clean_datasets.py',\n",
    "                                arguments = ['--folder', dengueAI],\n",
    "                                inputs=[dengue_train_ds.as_named_input('dengue_train'),dengue_test_ds.as_named_input('dengue_test')],\n",
    "                                outputs=[dengueAI],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "#Step 2, clean the raw datasets\n",
    "create_city_datasets = PythonScriptStep(name = 'Create City Datasets',\n",
    "                                source_directory = inputdata_folder,\n",
    "                                script_name = 'create_city_datasets.py',\n",
    "                                arguments = ['--folder', dengueAI],\n",
    "                                inputs=[dengueAI],\n",
    "                                compute_target = pipeline_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "\n",
    "print(\"Pipeline steps defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Experiment for the Data Prep Pipeline to Run In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built.\n",
      "Created step Create Clean Datasets [fabaa2f1][f673f654-ffa9-4e00-b841-d1a593975994], (This step will run and generate new outputs)Created step Create City Datasets [f383a5da][1c2e6f86-9a65-4164-b114-b529c6695fc8], (This step will run and generate new outputs)\n",
      "\n",
      "Submitted PipelineRun 1bffeb40-611f-49d9-a9c3-782bd47ed288\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-dataprep-pipeline/runs/1bffeb40-611f-49d9-a9c3-782bd47ed288?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "Pipeline submitted for execution.\n",
      "PipelineRunId: 1bffeb40-611f-49d9-a9c3-782bd47ed288\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/dengue-dataprep-pipeline/runs/1bffeb40-611f-49d9-a9c3-782bd47ed288?wsid=/subscriptions/fd2d8de8-17e1-4976-9906-fdde487edd5f/resourcegroups/AzureML-Learning/workspaces/Azure-ML-WS\n",
      "{'runId': '1bffeb40-611f-49d9-a9c3-782bd47ed288', 'status': 'Completed', 'startTimeUtc': '2020-11-16T14:57:47.768569Z', 'endTimeUtc': '2020-11-16T15:00:12.635925Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.1bffeb40-611f-49d9-a9c3-782bd47ed288/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=BXoSH2vH7vwdQDS%2FA2SAKa%2Fghai3ZEO4q83TsTga5wA%3D&st=2020-11-16T14%3A50%3A13Z&se=2020-11-16T23%3A00%3A13Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.1bffeb40-611f-49d9-a9c3-782bd47ed288/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=TA3KwNGB%2BY8RHJp2IChSx%2FZ5o6kdFc024zXTvzX0%2FdM%3D&st=2020-11-16T14%3A50%3A14Z&se=2020-11-16T23%3A00%3A14Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://azuremlws5740772408.blob.core.windows.net/azureml/ExperimentRun/dcid.1bffeb40-611f-49d9-a9c3-782bd47ed288/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=fD%2FlfqLZvZYjAMH4Is76RT5Q2t2%2BM4QDFJcRhoeeBRk%3D&st=2020-11-16T14%3A50%3A14Z&se=2020-11-16T23%3A00%3A14Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "#Construct the pipeline\n",
    "pipeline_steps=[create_clean_datasets,create_city_datasets]\n",
    "pipeline = Pipeline(workspace=ws,steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment=Experiment(workspace=ws, name='dengue-dataprep-pipeline')\n",
    "pipeline_run=experiment.submit(pipeline,regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
